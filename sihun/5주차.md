# Kubernetes Cluster Architecture & Core Components

## 1. Kubernetes 클러스터 아키텍처 개요

Kubernetes는 **컨테이너 기반 애플리케이션의 자동 배포, 스케줄링, 확장, 운영**을 위한 오픈소스 플랫폼이다. 이 시스템은 두 가지 핵심 구성요소로 나뉜다:

- **Control Plane (마스터 노드)**
- **Worker Nodes (작업자 노드)**

이 구조는 “컨테이너 선박(노드) + 관제 시스템(마스터)”에 비유

## 2. 주요 구성 요소 요약

### ✅ Control Plane 구성 요소

| 구성 요소 | 역할 |
| --- | --- |
| **etcd** | 클러스터 상태를 저장하는 고가용 키-값 DB (메타데이터 저장소) |
| **kube-apiserver** | 클러스터와의 모든 통신을 중개하는 중앙 API 게이트웨이 |
| **kube-scheduler** | 새로 생성된 Pod에 적절한 Node를 할당 |
| **kube-controller-manager** | 클러스터 상태를 모니터링하고 원하는 상태를 유지하도록 조치 |
| **cloud-controller-manager** | (클라우드 사용 시) 클라우드 리소스와의 연동을 담당 |

> ☁️ Kube-apiserver를 통해 모든 명령이 들어오며, etcd는 클러스터의 모든 상태를 보존한다.
>

### ✅ Node 구성 요소

| 구성 요소 | 역할 |
| --- | --- |
| **kubelet** | 각 노드에 상주하며, API 서버의 명령에 따라 컨테이너를 실행 및 상태 리포팅 |
| **kube-proxy** | 네트워크 트래픽을 관리하며, 서비스 IP/포트로 포워딩하는 역할 수행 |
| **Container Runtime** | 컨테이너 실행 환경 (예: Docker, containerd 등) |

> kubelet은 '선장'에 비유 → 노드 내 컨테이너를 관리하고 마스터와 통신.
>

## 3. 각 구성 요소 세부 기능 요약

### kube-apiserver

- Kubernetes API를 외부 및 내부에 노출
- 모든 REST 요청의 입출구
- 인증, 권한부여, 유효성 검사 등 수행

### etcd

- 클러스터 상태 저장소 (모든 오브젝트 정의 포함)
- 고가용 키-값 저장소
- `kubectl get` 명령 결과는 etcd에서 가져옴

### kube-scheduler

- 새 Pod에 알맞은 노드를 선택
- 고려 요소: 리소스 요구사항, 태인트/톨러레이션, affinity 등

### kube-controller-manager

- 여러 컨트롤러를 한 프로세스로 실행:
    - Node Controller
    - Replication Controller
    - Namespace Controller
    - Endpoints Controller 등
- 상태를 모니터링하고 수정함 (desired state 유지)

## kubelet & kube-proxy

| 항목 | 설명 |
| --- | --- |
| **kubelet** | 노드 내에서 Pod와 컨테이너 상태를 관리 |
| **kube-proxy** | 서비스 IP와 실제 Pod IP 간의 트래픽을 연결 |

## ⚙️ Container Runtime 비교 (Docker vs Containerd)

- Kubernetes는 CRI(Container Runtime Interface)를 사용해 다양한 런타임과 연동
- 사용 가능한 런타임: `Docker`, `containerd`, `CRI-O`
- v1.24 이후, `dockershim`이 제거되어 containerd 또는 CRI-O로 전환 필요

## 주요 명령어

```bash

kubectl get nodes
kubectl get pods -A
kubectl describe pod <pod-name>
kubectl get componentstatuses

```

etcd 데이터 확인 예시:

```bash

kubectl exec etcd-master -n kube-system -- etcdctl get / --prefix --keys-only

```

---

# Docker와 Containerd의 관계

### ✅ Docker는 종합 툴킷

Docker는 단순한 컨테이너 런타임이 아니라 여러 기능이 통합된 플랫폼:

- Docker CLI (명령어 인터페이스)
- Docker API
- 이미지 빌드 도구
- 인증/보안 모듈
- 컨테이너 런타임 (containerd + runc)

→ 쿠버네티스 입장에서 Docker는 "복합 플랫폼"이며, 쿠버네티스가 필요로 하는 것은 그중 *containerd* 같은 "런타임" 부분이다.

## 2. 쿠버네티스에서 Docker가 제거된 이유

| 항목 | 설명 |
| --- | --- |
| **초기** | Kubernetes는 Docker만 지원 (dockershim을 통해 연결) |
| **중간** | 다양한 런타임 지원 위해 CRI(Container Runtime Interface) 도입 |
| **결정** | v1.24부터 `dockershim` 제거 → Docker 지원 중단 |
| **결론** | Kubernetes는 이제 **containerd**, **CRI-O** 등의 CRI-호환 런타임만 직접 지원함 |

*하지만 Docker로 만든 이미지는 OCI 표준을 따르므로 containerd 등에서 계속 사용 가능하다.*

## 3. Container Runtime Interface (CRI)

### 목적

- 다양한 컨테이너 런타임과 쿠버네티스가 **공통 인터페이스(CRI)**를 통해 연동 가능하게 함.

### OCI(Open Container Initiative)

- 이미지 및 런타임 표준 제공 (image spec, runtime spec)
- Docker, containerd, CRI-O 등이 이를 따름

## 4. 주요 컨테이너 CLI 도구 비교

| CLI 도구 | 주체 | 용도 | 특성 |
| --- | --- | --- | --- |
| `ctr` | containerd | 디버깅 전용 | 기능 제한, 사용자 친화적 X |
| `nerdctl` | containerd | Docker 대체용 일반 CLI | Docker CLI와 유사, compose도 지원 |
| `crictl` | Kubernetes | CRI 기반 런타임 디버깅 | 포드/컨테이너 조회 가능, 디버깅 전용 |

---

### `ctr` 예시 (제한적, 디버깅 중심)

```bash

$ ctr images pull docker.io/library/redis:alpine
$ ctr run docker.io/library/redis:alpine redis

```

- 단순한 디버깅/테스트 목적
- 실서비스에는 적합하지 않음

---

### `nerdctl` 예시 (Docker 대체)

```bash

$ nerdctl run --name redis redis:alpine
$ nerdctl run --name webserver -p 80:80 -d nginx

```

- **Docker CLI와 거의 동일**
- docker-compose, 이미지 서명, 암호화 이미지, Lazy Pulling 등 최신 containerd 기능 사용 가능

---

### `crictl` 예시 (Kubernetes 기준 디버깅 도구)

```bash

$ crictl pull busybox
$ crictl ps -a
$ crictl logs <container-id>
$ crictl pods

```

- CRI 호환 런타임 모두 지원 (containerd, CRI-O 등)
- **Pod 레벨 디버깅 가능**
- Docker CLI와 유사한 문법

💡 *주의: crictl로 만든 컨테이너는 kubelet이 관리하지 않으면 삭제됨 (쿠버네티스 관점 외 컨테이너는 무효)*

## 5. 요약 비교

| 항목 | ctr | nerdctl | crictl |
| --- | --- | --- | --- |
| 소속 | containerd | containerd | Kubernetes |
| 목적 | 디버깅 | 일반 CLI | CRI 기반 디버깅 |
| 사용성 | 불편함 | Docker 대체 가능 | 디버깅 전용 |
| 컨테이너 생성 | O | O | 가능하지만 비추천 |
| 사용 권장도 | ❌ | ✅ | ✅ (디버깅 한정) |

# etcd 이해 및 활용

## 1. etcd란 무엇인가?

- **etcd**는 Kubernetes의 클러스터 상태 정보를 저장하는 **고가용성 분산 Key-Value 저장소**이다.
- **심플, 안전, 빠름 (Simple, Secure, Fast)** 이라는 특징을 가짐.
- 클러스터의 모든 오브젝트 정보(예: Pods, Configs, Secrets, ServiceAccount 등)가 저장됨.

## 2. Key-Value Store란?

### RDBMS vs Key-Value Store

| RDBMS | Key-Value Store (etcd) |
| --- | --- |
| 테이블 기반 (행/열 구조) | 개별 JSON/문서 단위 |
| 공통 필드가 필수 | 개별 엔트리마다 다른 구조 가능 |
| 새로운 정보가 전체 테이블에 영향 | 다른 엔트리에 영향 없음 |
| 복잡한 JOIN 연산 가능 | 단순 쿼리 및 트랜잭션 중심 |

etcd는 JSON 혹은 YAML과 같은 포맷으로 동작하며, **각 리소스를 독립적으로 관리**할 수 있다.

---

## 3. etcd 설치 및 실행

### 설치 절차 요약 (로컬 테스트 기준)

```bash

# 1. 다운로드
curl -L https://github.com/etcd-io/etcd/releases/download/v3.3.11/etcd-v3.3.11-linux-amd64.tar.gz -o etcd-v3.3.11-linux-amd64.tar.gz

# 2. 압축 해제
tar xzvf etcd-v3.3.11-linux-amd64.tar.gz

# 3. etcd 실행
./etcd

```

- 기본 포트: `2379`
- etcd 실행 후, 클라이언트(`etcdctl`)로 접속 가능

## 4. etcdctl 기본 사용법

### 🔑 Key-Value 저장/조회

```bash

# Key-Value 저장
./etcdctl set key1 value1        # (v2 방식)
./etcdctl put key1 value1        # (v3 방식)

# Key 조회
./etcdctl get key1               # (공통)

```

### 버전별 차이 (v2 vs v3)

| 항목 | 설명 |
| --- | --- |
| `etcd` 자체 버전 | 3.3.x 등 |
| `etcdctl` CLI 버전 | v2와 v3 모두 가능 |
| `API 버전` | CLI와 별도 설정 필요 (`ETCDCTL_API`) |

### 환경변수 설정 예시

```bash

# v3 API 사용
export ETCDCTL_API=3

# 저장
./etcdctl put name "etcd-user"

# 조회
./etcdctl get name

```

ETCDCTL_API=3 설정 없으면 기본은 v2 모드로 작동할 수 있음

## 5. etcd의 버전 역사

| 버전 | 주요 특징 |
| --- | --- |
| v0.1 (2013) | 초기 공개 |
| v2.0 (2015) | RAFT 합의 알고리즘 도입, 기본 안정 버전 |
| v3.0 (2017) | 성능 및 구조 개선, 새로운 API 도입 |
| 2018 | CNCF 프로젝트로 졸업 |

🧠 **v2 → v3**로의 전환은 커맨드 방식과 API 구조의 변경이 핵심이다.

## 🔍 6. 고가용성 구성과 RAFT 개념

- etcd는 분산 시스템으로, 클러스터 모드에서 **RAFT 합의 알고리즘**을 통해 데이터를 복제하고 동기화함
- 안정성을 위해 일반적으로 **3, 5개의 etcd 노드**가 권장됨
- 향후 CKA 고급 섹션(Cluster Maintenance, Troubleshooting 등)에서 자세히 다룸

---

# ETCD와 Kubernetes 클러스터 구성

## 1. ETCD란?

- **ETCD는 Kubernetes의 핵심 데이터 저장소**
- 클러스터의 **모든 상태 정보**(노드, 파드, 서비스, 시크릿 등)를 key-value 형식으로 저장한다.
- `kubectl` 명령으로 조회되는 정보는 모두 ETCD에서 가져온다.

## 2.ETCD와 클러스터 상태

- 클러스터의 모든 변경사항(노드 추가, 파드 배포 등)은 ETCD에 저장되어야만 **정상적으로 완료된 것**으로 간주됨.
- **중앙 집중식 상태 저장소**로써, 클러스터의 신뢰성과 일관성을 유지하는 역할을 한다.

## 3. 🛠️ Kubernetes 클러스터 배포 방식

| 방법 | 설명 | 특징 |
| --- | --- | --- |
| From Scratch | 수동으로 바이너리를 설치하여 클러스터 구성 | 유연성 ↑, 학습 목적에 적합 |
| kubeadm | 공식 CLI 툴로 자동화된 클러스터 초기화 | 운영에 적합, 인증서 자동 구성 |
- **실습환경에서는 kubeadm 사용**
- **시험에서는 둘 다 알아야 함** (특히 구성 요소가 어떻게 연결되는지)

## 4.인증서 (TLS Certificates)

- ETCD, API Server 등 주요 컴포넌트는 TLS 인증서를 이용하여 **보안 통신**을 한다.
- `kubeadm`은 인증서를 자동 생성하지만, 스크래치 설치 시 수동 구성 필요.
- ETCD는 보통 2379 포트를 사용하며, API 서버는 `-etcd-servers` 플래그로 연결 설정.

## 5. ETCD 탐색과 키 구조

- ETCD는 `/registry` 디렉토리를 루트로 하여 각 리소스를 저장함:

```

/registry/nodes/
/registry/pods/
/registry/deployments/
/registry/secrets/

```

- `etcdctl`로 키 열람 가능:

```bash

etcdctl get /registry --prefix --keys-only

```

## 6. 🧩 고가용성(HA) 구성에서의 ETCD

- HA 환경에서는 **다수의 마스터 노드에 각각 ETCD 인스턴스가 배포**됨.
- 이때 ETCD 인스턴스들은 서로를 인식하고 통신할 수 있도록 설정해야 함:

### 필수 파라미터

| 파라미터 | 설명 |
| --- | --- |
| `initial-cluster` | 클러스터를 구성할 멤버들의 리스트 |
| `initial-advertise-peer-urls` | 서로 통신할 수 있는 주소 |
| `initial-cluster-state` | `new` or `existing` (초기화 여부) |
- 이 설정들은 **클러스터 초기화 시점에만 유효**하며, 변경 시 주의 필요.

## 🔑 핵심 개념 정리

| 항목 | 설명 |
| --- | --- |
| ETCD | Kubernetes 상태 저장소 (key-value) |
| 클러스터 변경 반영 조건 | ETCD에 반영되어야 적용 완료 |
| 배포 방식 | 스크래치(수동) / kubeadm(자동) |
| 인증 | TLS 인증서 필요 |
| 포트 | 기본 2379 (ETCD) |
| 고가용성 | 여러 인스턴스 + 초기 클러스터 설정 필요 |

---

## kube-apiserver

### 1.  kube-apiserver란?

- **Kubernetes의 핵심 관리 구성 요소**
- 모든 클러스터 구성 요소들이 통신하는 **중앙 허브 (Single Source of Truth)**
- `kubectl`, `controller`, `scheduler`, `kubelet` 등 모든 컴포넌트는 **kube-apiserver를 통해서만** ETCD나 클러스터에 접근 가능

### 2. 기본 동작 흐름

1. **`kubectl` 실행 → kube-apiserver로 요청 전송**
2. kube-apiserver는 다음을 수행:
    - 🔐 **인증(Authentication)** 확인
    - ✅ **요청 유효성(Authorization, Admission)** 확인
3. 이후 요청 내용에 따라 처리:
    - ETCD에서 데이터 조회 / 생성 / 변경
    - 새로운 리소스를 생성 (예: Pod 생성 요청)
4. kube-apiserver는 결과를 **다른 구성요소들에게 전달**
    - `kube-scheduler`는 unscheduled Pod를 감지하고 스케줄링 수행
    - `kubelet`은 자신의 Node에 할당된 Pod를 실행
    - 모든 구성 요소는 상태를 kube-apiserver에 지속적으로 보고

### 3. API 직접 호출도 가능

- 꼭 `kubectl` 명령어가 아니라, **HTTP POST 요청으로도 Kubernetes API 호출 가능**
- ex) Pod 생성 시, 인증 및 유효성 검사는 동일하게 수행됨

### 4. 실행 방식 및 구성 위치

| 설정 방식 | 설명 |
| --- | --- |
| `kubeadm` 사용 시 | `kube-apiserver`는 `kube-system` 네임스페이스의 Pod로 배포됨 |
| 실행 파일 위치 | `/etc/kubernetes/manifests/kube-apiserver.yaml` |
| 비-kubeadm 수동 설치 | `systemd` 서비스로 등록되어 실행되며, 설정은 `/etc/systemd/system/kube-apiserver.service` 등에 위치할 수 있음 |

### 5. 인증/보안 구성 요소

- kube-apiserver는 **모든 요청에 대해 인증, 권한 검증, 보안 처리**를 수행
- 구성요소 간 통신은 **SSL/TLS 인증서 기반**으로 이뤄짐
    - 예: scheduler → apiserver / controller-manager → apiserver
- 인증서 위치 및 구성은 향후 "보안 섹션"에서 더 자세히 다룸

---

# kube-controller-manager

## 역할 (What is it?)

- 쿠버네티스 클러스터의 다양한 **컨트롤러(controller)**들을 실행하고 관리하는 핵심 컴포넌트
- 컨트롤러 = 클러스터 상태를 지속적으로 감시하고 원하는 상태로 **자동 조정**하는 관리자
- kube-apiserver와 연동하여 모든 조치를 수행함

## 주요 컨트롤러 예시 (모니터링 + 조정 기능)

| 컨트롤러 | 설명 |
| --- | --- |
| **Node Controller** | 노드 상태 감시 → 노드가 응답 없으면 관련 Pod 제거 |
| **Replication Controller** / **ReplicaSet Controller** | 원하는 수의 Pod를 항상 유지 |
| **Deployment Controller** | 버전 관리, 롤백, 롤링 업데이트 등 지원 |
| **Namespace Controller** | 네임스페이스와 관련 리소스 관리 |
| **PersistentVolume Controller** | PV/PVC 바인딩 및 상태 감시 |

## 🔁 작동 흐름 예시 (NodeController 기준)

1. 5초마다 각 노드의 상태를 체크 (Heartbeat)
2. 응답 없으면 일정 시간 후 `NotReady`로 표시
3. 5분 이상 응답 없으면 해당 노드의 Pod 강제 제거
4. ReplicaSet/Deployment가 다시 스케줄링하여 다른 노드에 Pod 생성

## ⚙️ 구성 및 설치 위치

### kubeadm으로 설치한 경우:

- `kube-controller-manager`는 **Static Pod**로 실행됨
- YAML 위치:

    ```
    
    /etc/kubernetes/manifests/kube-controller-manager.yaml
    
    ```


### 수동 설치한 경우:

- 시스템 서비스로 실행됨
- 확인 위치 예시:

    ```
    
    /etc/systemd/system/kube-controller-manager.service
    
    ```


## 🔧 주요 실행 옵션

| 옵션 | 설명 |
| --- | --- |
| `--controllers=*` | 실행할 컨트롤러 종류 (기본: 전부 실행) |
| `--node-monitor-period` | 노드 상태 점검 주기 |
| `--pod-eviction-timeout` | 노드 응답 없을 시 Pod 제거까지 대기 시간 |
| `--leader-elect=true` | 고가용성을 위한 리더 선출 기능 활성화 |

> kube-controller-manager는 “컨트롤 타워”
>
>
> 각 컨트롤러는 부서(노드 관리, 배포 관리 등)
>
> 클러스터 상태를 보고 계속해서 “자동 조정”함
>

---

## Kube Scheduler 핵심 정리

### 역할

- **Kube Scheduler**는 **Pod가 어느 Node에 배치될지 결정**하는 컴포넌트.
- 실제로 Pod를 Node에 배치하는 것은 **kubelet**의 역할.
- 마스터 노드에서 실행되며, `kube-system` 네임스페이스의 Pod로 존재.

### 스케줄링 동작 방식

1. **Filter 단계**
    - 리소스 부족(CPU, Memory 등), taint/toleration, nodeSelector 등 조건에 **맞지 않는 Node를 제거**.
2. **Score(우선순위) 단계**
    - 남은 Node에 점수(0~10)를 매겨 **가장 적합한 Node를 선택**.
    - 예: Pod를 배치하고 남는 자원이 많은 Node에 높은 점수 부여.
3. **최종 선정**
    - 점수가 가장 높은 Node에 Pod 스케줄링.

### 🔧 설치 및 구성

- `kubeadm`으로 설치 시:
    - `kube-scheduler`는 `/etc/kubernetes/manifests/kube-scheduler.yaml`에 정의된 **Static Pod** 형태로 배포됨.
    - 관련 설정은 해당 YAML 파일에서 확인 가능.
- 수동 설치 시:
    - Kubernetes 릴리스 페이지에서 `kube-scheduler` 바이너리 다운로드.
    - 스케줄러 구성 파일 지정하여 실행.

### ⚙️ 주요 설정 항목

- `-config`: 사용자 정의 스케줄러 정책 파일 지정
- 다양한 스케줄링 정책 예시:
    - nodeAffinity / podAffinity
    - taints and tolerations
    - topology spread constrain

---

## Kubelet 핵심 정리

### 역할

- Kubelet은 **노드에서 실행되는 에이전트**.
- **Pod의 생성, 실행, 모니터링을 책임**지는 핵심 컴포넌트.
- **Kube Scheduler**가 결정한 노드에 실제로 Pod를 배치함 (컨테이너 런타임 사용).
- 주기적으로 **Pod 상태를 API 서버에 보고**함.

### 기능 요약

- **PodSpec 수신** → 해당 컨테이너 이미지 풀링 및 실행
- **컨테이너 런타임** 호출 (예: Docker, containerd)
- **상태 보고**: Pod 상태를 API Server에 주기적으로 전달
- **헬스 체크 수행**: liveness/readiness probes 실행

### 설치 및 실행

- `kubeadm` 사용 시 자동 설치되지 않음
- **작업자 노드에 수동 설치 필요**
    - Kubelet 바이너리 설치 후 서비스로 등록
- 실행 확인: `ps aux | grep kubelet` 또는 `systemctl status kubelet`

### 관련 파일/옵션

- 주요 구성 파일:
    - `/etc/systemd/system/kubelet.service.d/10-kubeadm.conf`
    - `/var/lib/kubelet/config.yaml`
- 실행 시 주요 옵션:
    - `-config`: Kubelet 설정 파일 지정
    - `-kubeconfig`: API Server 접근용 인증정보 경로
    - `-container-runtime`: 사용할 컨테이너 런타임 지정

---

## Kube-Proxy 핵심 정리

### 역할

- **서비스 트래픽을 해당 백엔드 Pod로 전달**하는 역할
- 각 노드에서 실행되며 **클러스터 내 통신을 라우팅**함
- 가상 IP(ClusterIP)를 실제 Pod IP로 연결해주는 브리지 역할

### 동작 원리

| 기능 | 설명 |
| --- | --- |
| 서비스 발견 | 서비스가 생성되면 Kube-Proxy는 이를 감지 |
| 라우팅 규칙 설정 | 각 노드에 `iptables` 또는 `IPVS` 규칙을 설정 |
| 트래픽 전달 | 클러스터 IP로 도달한 트래픽을 적절한 백엔드 Pod로 전달 |
| 가상 컴포넌트 연결 | 서비스는 물리적 객체가 아니므로, 트래픽은 반드시 Kube-Proxy를 통해 백엔드 Pod로 전송됨 |

### Kube-Proxy 동작 예시

- 웹 앱이 DB 서비스를 호출할 때 `db-service`라는 이름이나 ClusterIP를 사용
- **Kube-Proxy는 이 요청을 포워딩하여 DB Pod에 연결**
- 내부적으로는 iptables 규칙이나 `ipvsadm`을 사용하여 연결

### 설치 방식

| 도구 | 설명 |
| --- | --- |
| kubeadm | `kube-proxy`를 **DaemonSet** 형태로 배포함 (각 노드 1개 Pod) |
| 수동 설치 | Kubernetes 공식 바이너리 다운로드 → systemd 서비스로 실행 가능 |
- 설치 후 확인: `kubectl get pods -n kube-system -o wide | grep kube-proxy`
- 실행 구조: `DaemonSet` → 모든 노드에 자동 배치

---

## Pods 핵심 요약

### 1. **Pod란 무엇인가**

- Kubernetes에서 **가장 작은 배포 단위**이다.
- 하나의 Pod는 **하나 이상의 컨테이너**를 포함할 수 있음.
- 대부분의 경우 **컨테이너 1개 ↔ Pod 1개** 구조 사용.
- 컨테이너는 Pod 안에서 **네트워크, 스토리지 공간을 공유**한다.
- Pod는 **같이 생성되고, 같이 사라지는** 단위이다.

### 2. **Pod의 역할 및 구조**

- 사용자 요청이 증가하면 **Pod 수를 늘려서(scale-out)** 대응한다.
    - 같은 Pod 내에 컨테이너를 추가하지 않고, **새로운 Pod**를 생성함.
- Pod는 애플리케이션의 **단일 인스턴스**를 나타낸다.
- 컨테이너 간 통신, 스토리지 공유 등은 Pod 내부에서 처리됨.

### 3. **다중 컨테이너 Pod 사용 사례**

- 일반적이지 않지만, 특정 상황에서는 유용:
    - ex) **웹 서버 + 파일 처리 도우미** 컨테이너를 같은 Pod에 배치
- 다중 컨테이너는:
    - **로컬 호스트 네트워크로 직접 통신**
    - **볼륨 공유** 가능
    - 생명주기를 함께함

### 4. **Docker 단독 vs Kubernetes Pod**

| 항목 | Docker 단독 | Kubernetes Pod |
| --- | --- | --- |
| 스케일링 | 직접 명령어로 여러 컨테이너 생성 | Pod 수 자동 확장 |
| 네트워크 연결 | 수동 구성 필요 | 동일 Pod 내 자동 연결 |
| 볼륨 공유 | 수동 설정 | 자동 공유 가능 |
| 상태 관리 | 수동 모니터링 및 종료 | 함께 시작/종료됨 |

### 5. **Pod 생성 방법**

- `kubectl run` 명령 사용

  예시: `kubectl run nginx --image=nginx`

- 기본적으로 `Docker Hub`에서 이미지를 가져옴
- Pod 목록 확인: `kubectl get pods`

### 6. **기본 운영 방식**

- 컨테이너 이미지는 **Docker Hub 또는 사설 레지스트리**에서 Pull함.
- Pod는 생성 직후 상태가 `ContainerCreating` → `Running`으로 변경됨.
- 생성된 Pod는 **내부에서는 접근 가능**하나, 외부 접근을 위해선 **Service 리소스 설정** 필요.

---

## YAML을 이용한 Pod 생성

### 1. Kubernetes YAML 정의 파일의 기본 구조

모든 쿠버네티스 리소스(YAML 정의)는 다음 4개의 **루트 필드**로 구성된다:

| 필드명 | 설명 |
| --- | --- |
| `apiVersion` | 리소스를 생성할 때 사용할 Kubernetes API의 버전 (예: `v1`, `apps/v1`) |
| `kind` | 생성할 리소스의 종류 (예: `Pod`, `Deployment`, `Service`) |
| `metadata` | 리소스의 메타 정보 (이름, 라벨 등) |
| `spec` | 리소스의 구체적인 사양 (컨테이너 정보 등) |

### 2. 각 필드의 상세 설명

### `apiVersion`

- 생성할 리소스에 따라 다름
- 예시:
    - `Pod` → `v1`
    - `Deployment` → `apps/v1`

### `kind`

- 생성할 리소스 타입 명시
    - 예: `Pod`, `Service`, `Deployment`, `ReplicaSet` 등

### `metadata`

- `name`: 객체 이름 (예: `my-app-pod`)
- `labels`: key-value 형식의 태그로 분류 및 선택에 활용

```yaml

metadata:
  name: my-app-pod
  labels:
    app: myapp
    tier: frontend

```

### `spec`

- 리소스의 동작 정의
- `Pod`에서는 주로 다음과 같은 구성 사용:

```yaml

spec:
  containers:
    - name: nginx-container
      image: nginx

```

> containers는 리스트 형식이며, 여러 개의 컨테이너도 포함 가능
>

### 3. Pod YAML 정의 예시

```yaml

apiVersion: v1
kind: Pod
metadata:
  name: my-app-pod
  labels:
    app: myapp
spec:
  containers:
    - name: nginx-container
      image: nginx

```

### 4. YAML 적용 및 확인

- YAML로 리소스를 생성:

```bash

kubectl apply -f pod-definition.yaml

```

- 생성된 Pod 확인:

```bash

kubectl get pods

```

- Pod 상세 정보 확인:

```bash

kubectl describe pod my-app-pod

```

### 5. YAML 작성 시 주의할 점

- **들여쓰기** 중요: YAML은 들여쓰기가 문법이다
- `metadata` 아래의 `name`, `labels`는 **같은 depth**
- `containers`는 **리스트(-)** 형식이며, 각 항목은 `name`, `image` 등의 하위 속성 포함

---

### 1. **쿠버네티스 컨트롤러란?**

- 쿠버네티스의 “두뇌” 역할
- 클러스터 상태를 지속적으로 **모니터링하고**, 설정된 **원하는 상태(desired state)**에 맞게 조정함

### 2. **Replication Controller (RC):**

### 개념

- 지정된 수의 **포드(Pod)** 인스턴스를 유지하는 역할
- 앱이 죽거나 고장 나면 자동으로 새 포드를 생성하여 **고가용성** 보장

### 사용 예시

- 포드 하나가 죽으면 새로운 포드를 생성하여 계속 실행
- 여러 포드를 실행해 **로드 밸런싱(부하 분산)** 가능

### 정의 파일 구조 (RC-definition.yaml)

- 구성 요소: `apiVersion`, `kind`, `metadata`, `spec`
- spec 안에:
    - `replicas`: 몇 개의 포드를 유지할지
    - `template`: 어떤 포드를 만들지 정의 (포드 정의 중첩)

### 명령어

- 생성: `kubectl create -f RC-definition.yaml`
- 조회: `kubectl get rc`, `kubectl get pods`

### 3. **ReplicaSet (RS):**

### 개념

- RC의 **업그레이드 버전**
- 더 많은 **선택기 기능**과 **유연성** 제공
- 현재는 RC보다 RS 사용이 **권장됨**

### 구조상의 차이점

- `apiVersion`이 `apps/v1`
- **selector** 필드가 **필수**
    - 어떤 라벨을 가진 포드를 관리할지 명시
- 이전에 만들어둔 포드도 레이블이 일치하면 포함 가능
- RC와 구조는 비슷하되, selector와 label 매칭이 중요

### 스케일링 방법

1. 정의 파일 수정 후 `kubectl replace -f <파일명>`
2. `kubectl scale --replicas=6 -f <파일명>` 또는 `-replicas=6 replicaset/<이름>`

### 명령어 요약

- 생성: `kubectl create -f RS-definition.yaml`
- 조회: `kubectl get rs`, `kubectl get pods`
- 삭제: `kubectl delete rs <이름>`
- 업데이트: `kubectl replace -f <파일명>`
- 스케일 조정: `kubectl scale --replicas=6 -f <파일명>`

### 4. **라벨과 선택기의 중요성**

- 쿠버네티스는 **라벨(label)**을 기반으로 포드를 식별하고 그룹화함
- ReplicaSet은 이 라벨을 통해 **어떤 포드를 관리할지 판단**
- 따라서 `spec.selector.matchLabels`와 `template.metadata.labels`는 일치해야 함

## 요약

| 항목 | 내용 |
| --- | --- |
| 문제 | 단일 포드는 장애 시 앱 중단 위험 |
| 해결 | RC 또는 RS를 사용해 복제본 유지 |
| 핵심 기능 | 포드 자동 생성, 로드 분산, 장애 대응 |
| RS vs RC | RS가 더 현대적이며 selector 등 확장성 제공 |
| 핵심 구조 | 정의 파일 내 `spec.replicas`, `spec.template`, `spec.selector` |
| 관련 명령어 | `kubectl create/get/delete/replace/scale` |

---

# Deployment

## Deployment란?

> Deployment는 Kubernetes에서 애플리케이션 배포, 업그레이드, 롤백, 스케일링 등을 자동으로 관리하는 상위 개념 리소스이다.
>

### 주요 기능

| 기능 | 설명 |
| --- | --- |
| **스케일링** | 여러 개의 Pod 인스턴스를 생성해 고가용성 보장 |
| **롤링 업데이트** | Pod를 순차적으로 업데이트하여 다운타임 없이 버전 변경 가능 |
| **롤백** | 문제가 생기면 이전 버전으로 손쉽게 되돌릴 수 있음 |
| **일시 중지/재개** | 일시적으로 변경 적용을 멈췄다가, 나중에 다시 실행 가능 |

## 왜 Deployment를 사용하는가?

| 상황 | Deployment로 해결 가능 |
| --- | --- |
| 웹 서버를 여러 개 띄워야 할 때 | ReplicaSet을 자동 생성하여 다중 인스턴스 관리 |
| 새 버전으로 점진적 업데이트 | 롤링 업데이트 기능 제공 |
| 오류 발생 시 이전 상태로 복귀 | 손쉬운 Rollback 기능 |
| 여러 설정 변경을 한 번에 적용 | Pause → 수정 → Resume 방식 지원 |

## Deployment의 구조

Deployment YAML 파일은 다음과 같은 구조로 구성된다:

```yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app-deployment
  labels:
    app: web-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web-app
  template:
    metadata:
      labels:
        app: web-app
    spec:
      containers:
        - name: nginx
          image: nginx:1.25

```

### 주요 필드 설명:

- `replicas`: 원하는 Pod의 개수
- `selector`: 어떤 라벨을 가진 Pod를 관리할지 지정
- `template`: 실제로 배포할 Pod의 템플릿 (spec 포함)
- `containers`: 컨테이너 목록과 이미지

Deployment는 내부적으로 ReplicaSet을 자동 생성하며, ReplicaSet은 다시 Pod를 생성함

## Deployment 생성 및 확인 명령어

```bash

# 배포 생성
kubectl apply -f deployment.yaml

# 생성된 배포 리소스 확인
kubectl get deployments

# 생성된 ReplicaSet 확인
kubectl get rs

# 생성된 Pod 확인
kubectl get pods

```

## Deployment가 관리하는 전체 구조

```

Deployment
  └── ReplicaSet (자동 생성)
        └── Pod (복수 개)

```

## 요약 정리

- Deployment는 **실제 운영 환경에서 앱을 안정적으로 배포**하는 핵심 도구
- ReplicaSet과 Pod을 **자동으로 생성하고 제어**
- 롤링 업데이트, 롤백, 스케일링, 일시정지/재개 등의 고급 기능 제공
- YAML 구조는 ReplicaSet과 유사하지만 상위 개념

---

# 서비스

- **Pod는 일시적인 존재**라서 IP가 계속 바뀜 → **안정적인 접근**을 보장하기 위해 **Service 객체가 필요**함.
- **Service는 논리적 Pod 그룹에 고정 IP/도메인을 부여**하고, 로드밸런싱까지 담당함.

### 주요 서비스 유형

| 서비스 유형 | 설명 |
| --- | --- |
| **ClusterIP** | 기본값. 클러스터 내부에서만 접근 가능. 내부 통신용 (ex. frontend → backend) |
| **NodePort** | 노드의 특정 포트를 통해 외부에서 접근 가능. 개발/테스트 환경에서 주로 사용 |
| **LoadBalancer** | 클라우드 환경에서 외부 로드밸런서를 통해 접근. 서비스 IP를 외부에 노출 |
| **ExternalName** | DNS 이름을 외부 서비스로 매핑함 (내부 DNS → 외부 서비스 주소) |

### NodePort 예시 정리

- 서비스 구조:

    ```
    
    Client → [NodeIP:30008] → Service → Pod(TargetPort:80)
    
    ```

- YAML 정의:

    ```yaml
    
    apiVersion: v1
    kind: Service
    metadata:
      name: my-service
    spec:
      type: NodePort
      selector:
        app: my-app
      ports:
        - port: 80
          targetPort: 80
          nodePort: 30008
    
    ```

- 핵심 구성요소:
    - `selector`: 어떤 Pod들과 연결할지 결정 (`app=my-app`)
    - `targetPort`: 실제 Pod의 컨테이너가 리스닝 중인 포트
    - `port`: 클러스터 내부 서비스 포트
    - `nodePort`: 외부에서 접근 가능한 포트 (기본 범위 30000~32767)

### 동작 흐름

1. 사용자가 `NodeIP:NodePort`로 요청을 보냄
2. 쿠버네티스가 해당 NodePort를 서비스로 라우팅
3. 서비스는 label selector로 일치하는 Pod를 찾아 요청 전달
4. Pod 여러 개라면 **자동 로드밸런싱(Random 방식)** 적용
5. Pod가 클러스터 내 여러 노드에 분산되어 있어도 **동일 포트 번호**로 접근 가능

### 특징

- 고가용성을 위해 Pod가 여러 개일 경우, Service가 자동으로 트래픽을 분산
- Service가 바라보는 Pod가 없어지거나 새로 생겨도 자동으로 업데이트됨
- 별도 설정 없이 **클러스터 전역에서 동일한 방식으로 접근 가능**
- `LoadBalancer`는 실서비스 클라우드 환경에서 주로 사용

---

# ClusterIP란?

**ClusterIP**는 쿠버네티스에서 가장 기본이 되는 서비스 유형으로,

**클러스터 내부의 Pod 간 통신을 안정적으로 연결**해주는 내부 IP 기반의 서비스입니다.

## 왜 ClusterIP가 필요한가?

- **Pod는 IP가 고정되어 있지 않다**: Pod가 재시작되거나 재배포되면 IP가 바뀜
- 따라서 **직접 IP를 이용해 통신하면 불안정**함
- 이를 해결하기 위해, **Service를 통해 Pod들을 묶고, 고정된 접근 지점(IP/이름)을 제공**함

## 예시: 풀스택 웹 애플리케이션의 구조

| 계층 | 역할 | Pod 예시 | 필요 연결 대상 |
| --- | --- | --- | --- |
| 프론트엔드 | 사용자 요청 처리 | `frontend-pod` | `backend` 서비스 |
| 백엔드 | 비즈니스 로직 처리 | `backend-pod` | `redis`, `mysql` 서비스 |
| 캐시 | 임시 데이터 저장 | `redis-pod` | - |
| DB | 영구 데이터 저장 | `mysql-pod` | - |

> 백엔드는 Redis와 MySQL에 접근해야 하고, 프론트엔드는 백엔드에 접근해야 함.
>
>
> 이 모든 연결을 **ClusterIP 서비스로 구성**하여 안정적인 통신을 보장함.
>

## ClusterIP 서비스 정의 파일 예시

```yaml

apiVersion: v1
kind: Service
metadata:
  name: backend
spec:
  selector:
    app: backend
  ports:
    - port: 80         # 서비스가 노출하는 포트
      targetPort: 80   # Pod 내부 컨테이너가 리스닝하는 포트
  type: ClusterIP      # 생략 가능 (기본값)

```

- `metadata.name`: 서비스 이름 → 다른 Pod에서 `backend`라는 이름으로 접근 가능
- `selector`: 해당 라벨을 가진 Pod들을 선택해 연결
- `ports.port`: 클러스터 내에서 이 포트를 통해 접근 가능
- `ports.targetPort`: 실제 Pod 내부의 포트

## ClusterIP의 특징 요약

| 항목 | 내용 |
| --- | --- |
| 기본값 여부 | O (서비스 유형을 지정하지 않으면 자동 ClusterIP로 생성됨) |
| 접근 범위 | 클러스터 내부에서만 접근 가능 |
| 사용 목적 | **Pod 간 통신** (프론트 → 백엔드, 백엔드 → DB 등) |
| 장점 | 유동적인 Pod IP를 추상화하여 **고정된 이름과 IP로 통신** 가능 |
| 라운드로빈 방식 | 여러 Pod가 연결되어 있을 경우, 요청은 무작위/라운드로빈으로 분산됨 |

## 생성과 확인 명령어

```bash

kubectl apply -f backend-service.yaml    # 서비스 생성
kubectl get services                     # 서비스 목록 확인

```

---

## LoadBalancer란?

> 클라우드 환경에서 애플리케이션을 외부에 노출하는 가장 간편한 방법
>
- `NodePort`는 클러스터 외부에서 접근 가능하지만,
    - **IP + Port 조합이 복잡**
    - **사용자 경험이 떨어짐**
- `LoadBalancer`는 외부에서 **하나의 퍼블릭 IP**로 접근 가능하게 해주는 서비스 타입
    - 사용자는 **단일 URL 또는 IP**만 기억하면 됨

## 작동 방식

[사용자] → [클라우드 제공자의 Load Balancer (GCP, AWS, Azure)] → [Kubernetes 서비스] → [Pod]

- 클라우드 제공자(GCP, AWS 등)가 **퍼블릭 IP + 로드밸런서**를 자동 생성
- 해당 IP로 들어오는 요청을 적절한 Pod로 전달

## 구성 예시

```yaml

apiVersion: v1
kind: Service
metadata:
  name: frontend
spec:
  type: LoadBalancer
  selector:
    app: frontend
  ports:
    - port: 80         # 외부에서 접근할 서비스 포트
      targetPort: 8080 # 실제 컨테이너 내부 포트

```

- `type: LoadBalancer`만 설정하면 클라우드 환경에서는 로드밸런서 자동 생성
- `kubectl get svc` 명령어로 **EXTERNAL-IP** 확인 가능

## NodePort와의 차이점

| 항목 | NodePort | LoadBalancer |
| --- | --- | --- |
| 노출 방식 | 노드의 IP + 고정 포트 조합 | 퍼블릭 IP 자동 생성 |
| 사용자 경험 | IP:Port 입력 필요 | 도메인/IP 하나로 접근 가능 |
| 클라우드 통합 | 필요 없음 (모든 환경에서 작동) | 클라우드 환경(GCP, AWS 등) 필요 |
| 로드밸런싱 | 각 노드로 수동 분산 | 클라우드 로드밸런서 자동 분산 |

## 지원 환경

- **완전한 LoadBalancer 기능은 클라우드 플랫폼(GCP, AWS, Azure)에서만 동작**
- **Minikube, VirtualBox 같은 로컬 환경에서는 LoadBalancer 타입을 써도 실제 외부 IP는 할당되지 않음**
    - 이 경우 **NodePort처럼 동작함**
    - Minikube에선 `minikube tunnel` 명령어로 외부 IP 흉내 가

---

# Namespace

> 쿠버네티스 클러스터 내에서 리소스들을 논리적으로 격리하기 위한 방법
>
- 하나의 클러스터 안에서 여러 **팀, 환경(Dev, Prod 등)**의 리소스를 구분해 관리 가능
- 네임스페이스는 일종의 **가상 공간(집)**처럼 작동
- 각 네임스페이스는 고유한 **정책, 리소스 할당량, 권한**을 가질 수 있음

## 기본적으로 존재하는 namespace

| namespace | 설명 |
| --- | --- |
| `default` | 사용자가 아무 것도 지정하지 않으면 여기에 생성됨 |
| `kube-system` | 쿠버네티스 내부 컴포넌트 (DNS, CoreDNS, kube-proxy 등) |
| `kube-public` | 모든 사용자가 접근 가능한 리소스 (보통 공개 설정용) |

## 💡 비유: 이름과 집

- 집 안에서는 “마크”라고만 불러도 되지만

  집 밖에서는 “마크 스미스”, “마크 윌리엄스”처럼 전체 이름 필요

  → 네임스페이스도 마찬가지


```bash

# 동일 네임스페이스에선 단순한 이름 가능
curl http://db-service

# 다른 네임스페이스의 서비스에 접근 시
curl http://db-service.dev.svc.cluster.local

```

## 사용하는 이유

| 목적 | 설명 |
| --- | --- |
| **환경 분리** | `dev`, `staging`, `prod` 등 환경별 리소스 격리 |
| **팀 분리** | 팀별 네임스페이스로 역할 및 리소스 제한 설정 |
| **보안** | RBAC 정책을 네임스페이스별로 적용 가능 |
| **리소스 제한** | `ResourceQuota`, `LimitRange`로 할당량 제한 |

## 관련 명령어

| 명령어 | 설명 |
| --- | --- |
| `kubectl get pods` | 기본(default) 네임스페이스의 포드 조회 |
| `kubectl get pods -n dev` | `dev` 네임스페이스의 포드 조회 |
| `kubectl get pods --all-namespaces` | 전체 네임스페이스의 포드 조회 |
| `kubectl create namespace dev` | `dev` 네임스페이스 생성 |
| `kubectl config set-context --current --namespace=dev` | 현재 세션을 `dev` 네임스페이스로 설정 |

## 📄 YAML 예시: namespace 정의

```yaml

apiVersion: v1
kind: Namespace
metadata:
  name: dev

```

- 이 파일로 네임스페이스를 생성:

```bash
bash
복사편집
kubectl apply -f namespace-dev.yaml

```

## 🎯 Pod에 Namespace 명

```yaml

apiVersion: v1
kind: Pod
metadata:
  name: my-pod
  namespace: dev
spec:
  containers:
    - name: nginx
      image: nginx

```

> 정의 파일에서 metadata.namespace를 설정하면 명시된 네임스페이스에 생성됨
>

## 정리

| 개념 | 설명 |
| --- | --- |
| 네임스페이스란 | 클러스터 내 리소스의 논리적 구획 |
| 기본 네임스페이스 | `default`, `kube-system`, `kube-public` |
| 생성 방법 | `kubectl create namespace`, YAML |
| 접근 방식 | 동일 네임스페이스: 간단한 이름 / 다른 네임스페이스: FQDN |
| 사용 목적 | 격리, 보안, 자원제한, 팀/환경 분리 등 |
| DNS 이름 규칙 | `<서비스명>.<네임스페이스>.svc.cluster.local` |

---

## Imperative vs Declarative 접근법

| 항목 | 명령적 (Imperative) | 선언적 (Declarative) |
| --- | --- | --- |
| 방식 | **무엇을 어떻게 할지 명령어로 직접 지시** | **최종 상태만 선언, 방법은 시스템이 결정** |
| 유사한 사례 | 택시 기사에게 길을 직접 지시 | 목적지만 알려주고 자동으로 이동 |
| 도구 | `kubectl run`, `kubectl edit`, `kubectl set` | `kubectl apply`, YAML 매니페스트 사용 |
| 상태 기록 | 기록되지 않음 (히스토리만 남음) | Git 등에서 기록 및 관리 가능 |
| 에러 처리 | 상태 확인 없이 실행 → 충돌 발생 가능 | 존재 여부 확인 후 생성/업데이트 자동 처리 |
| 적합한 상황 | 빠르게 테스트하거나 단순 리소스 생성 시 | 복잡한 리소스 관리, 인프라 코드화, 협업 |

## 명령적 접근법 (Imperative)

### 특징

- 빠르게 명령어를 통해 리소스를 생성, 수정, 삭제
- 한 번 실행하면 끝 (YAML 파일 없음)
- 명령 예시:

    ```bash
    
    kubectl run nginx --image=nginx
    kubectl expose deployment my-app --port=80
    kubectl scale deployment my-app --replicas=3
    kubectl set image deployment/my-app nginx=nginx:1.20
    kubectl edit pod my-pod
    kubectl delete pod my-pod
    
    ```


### 단점

재현/협업 어려움 (어떻게 만들었는지 기록 없음)

- 실수 시 롤백이나 비교 불가
- 이미 있는 리소스에 대해 중복 생성 시 오류 발생

---

## 선언적 접근법 (Declarative)

### 특징

- 리소스의 “원하는 상태”를 YAML로 정의
- `kubectl apply -f file.yaml`로 생성/업데이트
- GitOps 및 IaC(Infrastructure as Code)에 적합
- 예시:

    ```yaml
    
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: nginx-deploy
    spec:
      replicas: 3
      template:
        spec:
          containers:
          - name: nginx
            image: nginx:1.20
    
    ```


### 명령 예시

```bash

kubectl apply -f deployment.yaml      # 생성 또는 업데이트
kubectl delete -f deployment.yaml     # 삭제
kubectl diff -f deployment.yaml       # 차이 비교

```

### 장점

- **변경 사항 추적 가능 (Git 관리)**
- **변화만 자동으로 반영됨 → idempotent**
- **복잡한 리소스 정의 가능 (multi-container, env 등)**

## 명령어 정리

| 목적 | 명령형 (Imperative) | 선언형 (Declarative) |
| --- | --- | --- |
| 리소스 생성 | `kubectl run`, `kubectl create` | `kubectl apply -f <file>` |
| 리소스 수정 | `kubectl edit`, `kubectl set image` | YAML 파일 수정 후 `kubectl apply -f` |
| 리소스 삭제 | `kubectl delete pod mypod` | `kubectl delete -f <file>` |
| 스케일링 | `kubectl scale deployment my-app --replicas=3` | YAML 수정 후 `apply` |

## 요약

> 명령적 접근법은 속도에 유리하지만 기록과 관리에 취약하다.
>
>
> **선언적 접근법은 예측 가능성과 협업에 유리하고, 유지보수가 용이하다.**
>
> **실무에선 선언적 방식을 기본으로, 명령적 방식은 실험과 디버깅 용도로 활용한다.**
>

---