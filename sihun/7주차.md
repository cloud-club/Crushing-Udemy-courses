# Kubernetes Scheduling 학습 정리

## 1. 스케줄러의 역할

Kubernetes에서 스케줄러는 생성된 Pod을 클러스터 내 적절한 Node에 배치하는 역할을 맡음.

기본적으로는 `kube-scheduler`가 자동으로 작동하지만, 필요에 따라 수동 스케줄링을 하거나 커스텀 스케줄러를 구성할 수 있음.

## 2. 수동 스케줄링 (Manual Scheduling)

### 2-1. `.spec.nodeName` 필드를 이용한 스케줄링

Pod을 생성할 때 명시적으로 `nodeName` 필드를 지정하면, kube-scheduler 없이도 원하는 노드에 Pod이 배치됨.

```yaml

apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
    - name: nginx
      image: nginx
  nodeName: node01

```

위와 같이 작성하면 `nginx`라는 Pod은 `node01` 노드에 직접 생성됨.

단, 이 방법은 Pod 생성 시점에만 가능하며, 이미 생성된 Pod에는 `nodeName`을 수정할 수 없음.

### 2-2. Binding 객체를 통한 수동 배정

이미 생성된 Pod에 대해 노드를 수동으로 지정하고자 할 경우, Binding 리소스를 생성하여 API 서버에 직접 요청을 보내는 방식이 사용됨.

```json

POST /api/v1/namespaces/default/pods/nginx/binding
{
  "apiVersion": "v1",
  "kind": "Binding",
  "metadata": {
    "name": "nginx"
  },
  "target": {
    "apiVersion": "v1",
    "kind": "Node",
    "name": "node02"
  }
}

```

이 요청은 실제 kube-scheduler가 내부적으로 수행하는 바인딩 작업을 사용자가 직접 수행하는 것과 동일한 방식임.

## 3. DaemonSet

DaemonSet은 클러스터 내 **모든 노드에 동일한 Pod을 하나씩 배포**하는 데 사용되는 리소스임.

모니터링, 로그 수집, 네트워크 플러그인 등 노드별로 백그라운드 에이전트를 배포할 때 활용됨.

```yaml

apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: monitoring-daemon
spec:
  selector:
    matchLabels:
      app: monitoring-agent
  template:
    metadata:
      labels:
        app: monitoring-agent
    spec:
      containers:
        - name: monitoring-agent
          image: monitoring-agent

```

DaemonSet은 노드 수가 늘어나면 자동으로 새 노드에도 Pod을 하나 배포하며, 노드가 삭제되면 해당 Pod도 함께 사라짐.

## 4. Multiple Schedulers (다중 스케줄러)

Kubernetes는 기본 스케줄러 외에도 여러 개의 사용자 정의 스케줄러를 동시에 운영할 수 있도록 지원함.

Pod에 `.spec.schedulerName`을 지정하면 해당 Pod은 기본 스케줄러가 아닌 지정된 스케줄러에 의해 처리됨.

```yaml

spec:
  schedulerName: my-custom-scheduler

```

커스텀 스케줄러는 아래와 같이 별도의 Pod으로 실행 가능하며, `--scheduler-name` 옵션을 통해 이름을 지정함.

```yaml

apiVersion: v1
kind: Pod
metadata:
  name: my-custom-scheduler
  namespace: kube-system
spec:
  containers:
    - name: kube-scheduler
      image: k8s.gcr.io/kube-scheduler-amd64:v1.11.3
      command:
        - kube-scheduler
        - --scheduler-name=my-custom-scheduler

```

이 구조는 서로 다른 기준으로 Pod을 스케줄링할 필요가 있는 경우 유용하게 활용됨.

## 5. 스케줄링 이벤트 및 로그 확인

### 5-1. 이벤트 확인

```bash

kubectl get events

```

Pod이 특정 노드에 할당되었는지, 이미지가 정상적으로 Pull 되었는지 등 다양한 상태를 확인할 수 있음.

### 5-2. 로그 확인

```bash

kubectl logs my-custom-scheduler -n kube-system

```

스케줄러 동작 로그를 확인함으로써, 리더 선출 과정이나 스케줄링 실패 원인 등을 추적할 수 있음.

| 항목 | 설명 |
| --- | --- |
| 수동 스케줄링 | `nodeName` 지정 또는 Binding 객체 사용 |
| DaemonSet | 모든 노드에 Pod을 하나씩 자동 배포 |
| 다중 스케줄러 | 커스텀 스케줄러를 동시에 운영하고 Pod에서 선택 가능 |
| 이벤트 및 로그 | 배치 과정 및 상태 확인 가능 |

---

# 라벨과 선택자 (Labels & Selectors)

라벨(Label)과 선택자(Selector)는 Kubernetes에서 객체들을 그룹화하거나 필터링할 때 사용하는 기본적인 도구이다.

수많은 Pod, ReplicaSet, Deployment 등 다양한 리소스가 생성되는 환경에서 이들을 효과적으로 관리하기 위해 반드시 필요한 개념이다.

라벨은 객체에 부착되는 키-값 쌍이며, 선택자는 이러한 라벨을 기준으로 특정 조건을 만족하는 객체를 찾아내는 데 사용된다.

예를 들어, 다음과 같은 라벨을 붙일 수 있다.

```yaml

metadata:
  labels:
    app: myapp
    tier: frontend
    env: production

```

위처럼 하나의 리소스에 여러 개의 라벨을 붙일 수 있으며, 선택자는 이 중 원하는 조건만 골라 필터링할 수 있다.

```bash

kubectl get pods -l app=myapp

```

위 명령은 `app=myapp` 라벨이 붙은 Pod만 조회하는 예이다.

라벨은 단순히 보기 좋게 분류하는 수준을 넘어서, Kubernetes 내부 기능들이 서로 연결될 수 있게 해주는 기준점 역할을 한다.

예를 들어 ReplicaSet과 Pod의 관계도 라벨과 선택자로 연결된다.

ReplicaSet은 생성할 Pod의 템플릿에 라벨을 붙이고, 선택자(selector)에서 해당 라벨을 가진 Pod을 감지하여 관리한다.

```yaml

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: frontend-replicaset
spec:
  replicas: 3
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
        - name: frontend
          image: nginx

```

여기서 중요한 포인트는 다음과 같다.

- `spec.selector.matchLabels` → 어떤 Pod을 이 ReplicaSet이 관리할 것인지 지정
- `spec.template.metadata.labels` → 생성할 Pod에 어떤 라벨을 붙일지 명시

이 둘이 일치하지 않으면 ReplicaSet이 Pod을 생성하거나 관리하지 못하므로, 초보자들이 자주 실수하는 지점이다.

서비스(Service)도 마찬가지이다. Service는 선택자를 이용해 연결할 Pod을 찾는다.

```yaml

apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: myapp
  ports:
    - port: 80
      targetPort: 8080

```

위 예시에서 이 서비스는 `app=myapp` 라벨이 붙은 Pod에게 요청을 전달하게 된다.

선택자는 단순한 일치(matchLabels) 외에도 복잡한 조건(`matchExpressions`)을 사용할 수 있다.

```yaml

selector:
  matchExpressions:
    - key: env
      operator: In
      values:
        - production
        - staging

```

이를 통해 다양한 조합으로 원하는 Pod을 필터링할 수 있다.

마지막으로 주석(annotation)은 라벨과 달리, 객체에 부가적인 설명 정보를 제공하기 위해 사용된다.

라벨은 Kubernetes 시스템 동작에 직접적인 영향을 주지만, 주석은 주로 정보 전달이나 문서화 용도로 활용된다.

예를 들면 다음과 같다.

```yaml

metadata:
  annotations:
    contact: devops@example.com
    version: v1.2.3

```

정리하자면, 라벨은 객체를 식별하고 선택하는 핵심 수단이며, 선택자는 라벨 기반으로 특정 객체들을 필터링할 수 있는 도구이다.

ReplicaSet, Service 등 주요 Kubernetes 리소스는 이러한 라벨과 선택자를 통해 상호 연결된다.

주석은 시스템 동작에 영향을 주지 않지만, 필요한 정보를 메타데이터 형태로 저장하는 데 유용하다.

---

# Taints와 Tolerations

이번 강의에서는 Kubernetes에서 특정 노드에 어떤 Pod이 배치될 수 있는지를 제한하는 **Taint(오점)**와 **Toleration(관용)** 개념에 대해 다룬다.

Taint는 노드에 설정되는 일종의 "거부 조건"이고, Toleration은 Pod이 그 조건을 "참을 수 있는지"를 나타내는 속성이다.

즉, 노드가 특정 Pod을 거부하고 싶을 때는 Taint를 사용하고, 그 거부 조건을 특정 Pod이 감내할 수 있도록 하려면 Toleration을 사용하면 된다.

## Taint와 Toleration의 비유

노드를 사람, Pod을 벌레에 비유할 수 있다.

사람은 벌레가 달라붙는 걸 막기 위해 방충제를 뿌린다. 이것이 Taint이다.

어떤 벌레는 이 방충제를 견디지 못하고 사람에게 붙지 못한다. 하지만 어떤 벌레는 이 냄새를 참을 수 있다. 이 경우, 그 벌레에게는 Toleration이 있는 것이다.

## 기본 동작

기본적으로는 모든 Pod이 어떤 노드에나 배치될 수 있다.

하지만 특정 노드에는 특정 앱만 배치하고 싶은 경우, 해당 노드에 Taint를 설정함으로써 일반적인 Pod 배치를 막을 수 있다.

예를 들어, `node1`이라는 노드에는 특정 앱에만 열려 있어야 한다고 가정할 수 있다.

이 경우 다음 명령어로 Taint를 추가할 수 있다.

```bash

kubectl taint nodes node1 app=blue:NoSchedule

```

위 명령은 `app=blue`라는 키-값 쌍과 함께 `NoSchedule` 효과를 가진 Taint를 `node1`에 추가한다.

이제 `node1`에는 기본적으로 어떤 Pod도 배치되지 않는다.

## Toleration 추가 방법

특정 Pod이 이 Taint를 견디도록 하려면, Pod 정의에 Toleration을 추가해야 한다.

예시는 다음과 같다.

```yaml

apiVersion: v1
kind: Pod
metadata:
  name: d-pod
spec:
  containers:
    - name: d-container
      image: nginx
  tolerations:
    - key: "app"
      operator: "Equal"
      value: "blue"
      effect: "NoSchedule"

```

이 설정을 통해 해당 Pod은 `app=blue:NoSchedule` Taint가 있는 노드에도 배치될 수 있게 된다.

## Taint 효과의 종류

Taint에는 다음과 같은 세 가지 효과(effect)가 존재한다.

1. `NoSchedule`

   → 해당 Taint를 견디지 않는 Pod은 절대 배치되지 않음

2. `PreferNoSchedule`

   → 가급적 피하지만 반드시 막지는 않음

3. `NoExecute`

   → 견디지 못하는 Pod이 이미 배치돼 있다면 퇴거(Evicted)됨


예를 들어 `NoExecute` 효과를 가진 Taint를 설정하면, 그 조건을 견디지 못하는 기존 Pod이 노드에서 제거된다.

이를 통해 특정 노드를 "특정 Pod 전용"으로 설정할 수 있다.

## 마스터 노드의 기본 Taint

초기 Kubernetes 클러스터가 구성되면, 마스터 노드에는 자동으로 다음과 같은 Taint가 추가되어 있다.

```bash

node-role.kubernetes.io/master:NoSchedule

```

이로 인해 일반 Pod은 마스터 노드에 배치되지 않으며, 이는 운영상 권장되는 기본 설정이다.

해당 Taint는 `kubectl describe node <마스터 노드 이름>` 명령을 통해 확인할 수 있다.

## 정리

- **Taint**는 노드에 설정되며, 특정 조건을 견디지 못하는 Pod의 배치를 막는다.
- **Toleration**은 Pod에 설정되며, 특정 Taint를 견딜 수 있게 한다.
- 이를 통해 특정 노드를 특정 애플리케이션 전용으로 설정하거나, 시스템 안정성을 확보할 수 있다.
- Taint가 Pod의 배치를 “허용하는” 기준은 아니며, “거부”의 기준이라는 점에 유의해야 한다. 특정 노드에 반드시 배치하고 싶다면, 이후 배울 `Node Affinity`를 활용해야 한다.

---

# NodeSelector와 Node Affinity

## NodeSelector: 기본적인 노드 지정 방법

Pod을 특정 노드에 배치해야 할 필요가 있을 때 사용하는 가장 단순한 방법은 `NodeSelector`이다.

이는 노드에 미리 붙여둔 라벨을 기준으로 Pod이 어떤 노드에서 실행되어야 할지를 제한하는 방식이다.

예를 들어 다음과 같은 클러스터가 있다고 가정한다.

- node1: 고성능(large) 노드
- node2, node3: 저사양(small) 노드

`C`라는 데이터 처리 전용 Pod이 반드시 `node1`에서만 실행되어야 한다면, 먼저 해당 노드에 라벨을 붙여야 한다.

```bash

kubectl label nodes node1 size=large

```

이제 Pod 정의 파일에 다음과 같이 `nodeSelector`를 추가한다.

```yaml

spec:
  nodeSelector:
    size: large

```

이 설정을 통해 `C` Pod은 `size=large` 라벨이 있는 노드, 즉 `node1`에만 배치된다.

### 한계점

`NodeSelector`는 단순한 키-값 조건 하나만 사용할 수 있다는 제한이 있다.

예를 들어 “중간 노드나 큰 노드”처럼 복수 조건을 적용하거나, “작지 않은 노드”처럼 부정 조건을 쓸 수 없다.

이를 해결하기 위해 도입된 것이 **Node Affinity**이다.

## Node Affinity: 고급 노드 선택 기능

`Node Affinity`는 `NodeSelector`보다 더 다양한 조건을 사용하여 Pod의 배치 정책을 정의할 수 있게 해주는 기능이다.

형식상으로는 `affinity.nodeAffinity` 필드 아래 여러 조건을 설정할 수 있으며, `NodeSelectorTerms` 형태로 조건식을 구성한다.

예를 들어, `size=large` 또는 `size=medium`인 노드에 Pod을 배치하려면 다음과 같이 작성한다.

```yaml

spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: size
                operator: In
                values:
                  - large
                  - medium

```

이 설정은 Pod이 스케줄링되는 시점에만 적용되며, 한 번 배치되면 노드 라벨이 바뀌더라도 Pod은 계속 실행된다.

## Node Affinity의 구성 요소

Node Affinity는 다음과 같은 구성으로 이루어진다.

- **type** (적용 시점)
    - `requiredDuringSchedulingIgnoredDuringExecution`: 필수 조건. 만족하지 않으면 스케줄링 자체가 안 됨.
    - `preferredDuringSchedulingIgnoredDuringExecution`: 선호 조건. 가능하면 만족시키되, 안 되면 다른 노드에도 배치 가능.
- **matchExpressions** (조건)
    - `key`: 라벨 키 (예: size)
    - `operator`: 조건 종류 (`In`, `NotIn`, `Exists`, `DoesNotExist` 등)
    - `values`: 비교 대상 값 목록

예를 들어 "작은 노드는 제외하고 싶다"는 조건은 아래와 같이 설정한다.

```yaml

- key: size
  operator: NotIn
  values:
    - small

```

또는 "size 라벨이 존재하는 노드에만 배치하라"는 조건은 `Exists` 연산자를 사용할 수 있다.

```yaml

- key: size
  operator: Exists

```

## 실행 중 라벨이 바뀌는 경우

Node Affinity는 **스케줄링 시점에만 적용**된다.

따라서 노드 라벨이 변경되더라도, 이미 배치된 Pod은 계속 실행된다.

예를 들어, `size=large`인 노드에 배치된 Pod이 있다고 가정할 때, 해당 라벨이 삭제되더라도 Pod은 종료되지 않는다.

향후 Kubernetes에서는 실행 중에도 Affinity 조건을 적용할 수 있는 기능(`requiredDuringExecution`)을 도입할 예정이다.

이 기능이 활성화되면, 라벨 조건을 만족하지 않게 된 노드에서 실행 중인 Pod은 퇴거될 수 있다.

## 요약

| 기능 구분 | NodeSelector | Node Affinity |
| --- | --- | --- |
| 조건 복잡도 | 단일 키-값 조건만 가능 | AND/OR, 부정 조건, 존재 여부 등 가능 |
| 적용 시점 | 스케줄링 시점 | 스케줄링 시점 (`required`, `preferred`) |
| 실행 중 반응 | 라벨이 바뀌어도 영향 없음 | 현재는 동일, 미래에는 퇴거 조건으로 확장 예정 |
| 주 사용 목적 | 간단한 노드 제한 | 고급 노드 배치 전략 구현 |

NodeSelector는 단순한 정책을 정의할 때 유용하며, Node Affinity는 복잡한 조건이나 정책 기반 배치가 필요할 때 적합하다.

실제 운영 환경에서는 두 기능을 조합하여 상황에 맞는 배치 전략을 구성하는 것이 일반적이다.

---

# 스케줄링 응용 및 리소스 제약

## 오염(Taints) + 관(Tolerations) + 노드 친화성(Node Affinity)의 조합

Kubernetes 환경에서는 특정 Pod을 특정 노드에만 고정하고, 다른 Pod의 접근은 제한하는 경우가 자주 발생한다.

이런 요구사항을 충족하려면 아래 세 가지 기능을 적절히 조합해야 한다.

1. **Taints**: 노드에 설정하여, 특정 조건을 견디지 못하는 Pod의 배치를 거부함
2. **Tolerations**: Pod에 설정하여, 특정 Taint를 수용할 수 있게 허용함
3. **Node Affinity**: Pod이 어떤 라벨을 가진 노드에 배치될지 명시적으로 제어함

예를 들어, 노드와 Pod이 각각 색상(파랑, 빨강, 초록)으로 구분된다고 할 때 다음 조건을 만족시켜야 한다:

- 파란 Pod은 파란 노드에만 배치됨
- 파란 노드에는 파란 Pod 외에는 배치되지 않음

이를 위해 다음과 같이 구성할 수 있다:

- 파란 노드에 `taint` 추가: `kubectl taint nodes node1 color=blue:NoSchedule`
- 파란 Pod에 해당 `toleration` 추가
- 또한 파란 노드에 라벨 추가: `kubectl label nodes node1 color=blue`
- 파란 Pod에 `Node Affinity` 추가

이렇게 구성하면, 파란 Pod은 파란 노드에만 배치되며, 다른 Pod은 해당 노드에 접근할 수 없게 된다.

세 기능을 함께 사용하는 이유는 **완전한 고정 조건**을 만족시키기 위해서이다.

## 리소스 요청(Requests)과 제한(Limits)

Kubernetes 스케줄러는 Pod이 실행될 수 있는 노드를 결정할 때, Pod이 요청한 리소스와 노드의 가용 리소스를 비교하여 결정한다.

```yaml

resources:
  requests:
    memory: "512Mi"
    cpu: "500m"
  limits:
    memory: "1Gi"
    cpu: "1"

```

- **requests**: Pod이 최소한으로 보장받아야 할 자원
- **limits**: Pod이 사용할 수 있는 최대 자원

### CPU 단위

- `1`은 vCPU 1개를 의미하며, 클라우드 환경에서의 vCPU 또는 하이퍼스레드 1개와 같음
- `500m`은 0.5 vCPU를 의미하며, `m`은 milli(1/1000) 단위

### Memory 단위

- `Mi`는 **Mebibyte(2^20 byte)** 단위로, 1Mi = 1,048,576 byte
- `Gi`는 **Gibibyte(2^30 byte)**, `M` 또는 `G`는 SI 표준인 MB/GB와 다름

## 리소스 초과 시 동작

- CPU 사용량 초과 시 → **제한을 넘지 않도록 CPU 사용률이 자동 조절**됨 (Throttling)
- 메모리 사용량 초과 시 → **OOM(Out of Memory)** 발생 → 컨테이너가 종료됨

OOM 로그는 다음 명령으로 확인 가능하다.

```bash

kubectl describe pod <pod-name>

```

## Pod 내 여러 컨테이너 리소스 설정

Pod이 여러 개의 컨테이너를 포함할 경우, 각 컨테이너마다 개별적으로 리소스 요청 및 제한을 설정할 수 있다.

요청은 스케줄링에 영향을 주고, 제한은 런타임에서 실제 자원 사용을 제어한다.

## LimitRange: 기본 요청 및 제한 설정

Kubernetes는 기본적으로 Pod 생성 시 requests나 limits 설정이 없다. 이를 보완하기 위해 **LimitRange**를 사용할 수 있다.

LimitRange는 Namespace 단위로 적용되며, 컨테이너에 대한 기본값과 최대/최소치를 정의한다.

```yaml

apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-limit-range
spec:
  limits:
    - default:
        cpu: "500m"
      defaultRequest:
        cpu: "100m"
      max:
        cpu: "1"
      min:
        cpu: "100m"
      type: Container

```

- `default`: 설정하지 않은 경우 적용되는 최대값
- `defaultRequest`: 설정하지 않은 경우 적용되는 최소 보장값
- `min` / `max`: 수동으로 설정했을 때 유효 범위 제한

## ResourceQuota: Namespace 수준 자원 제한

여러 사용자가 같은 클러스터를 사용할 때, 한 사용자가 자원을 독점하지 못하도록 **ResourceQuota**를 설정할 수 있다.

```yaml

apiVersion: v1
kind: ResourceQuota
metadata:
  name: mem-cpu-quota
spec:
  hard:
    requests.cpu: "4"
    requests.memory: "4Gi"
    limits.cpu: "10"
    limits.memory: "10Gi"

```

위 설정은 해당 Namespace 내에서 요청된 CPU는 총 4개, 제한된 CPU는 총 10개를 넘지 못하게 한다.

모든 Pod의 합산 기준이며, 새로운 Pod 생성 시 이 범위를 초과하면 거부된다.

## 정리

| 개념 | 설명 |
| --- | --- |
| Taint & Toleration | 노드에서 특정 Pod을 거부/허용 |
| Node Affinity | Pod을 특정 노드에 선호 혹은 제한적으로 배치 |
| Resource Requests | 스케줄링 기준이 되는 최소 자원 보장 |
| Resource Limits | 런타임에서 자원 사용 상한 |
| LimitRange | Namespace 내 Pod 기본 리소스 설정 |
| ResourceQuota | 전체 Namespace에 자원 총량 제한 |

---

# DaemonSet

## DaemonSet이란?

**DaemonSet은 클러스터의 모든 노드에 Pod을 하나씩 배포하는 리소스**이다.

ReplicaSet이나 Deployment와 유사하게 여러 인스턴스를 배포하지만, 차이점은 **노드당 정확히 하나의 Pod만 실행된다는 점**이다.

클러스터에 새로운 노드가 추가되면 DaemonSet은 해당 노드에도 자동으로 새로운 Pod을 배포한다.

반대로 노드가 삭제되면 해당 노드에 있던 Pod도 자동으로 제거된다.

따라서 DaemonSet은 노드 수에 따라 동적으로 Pod을 유지해주는 구조이다.

## 사용 사례

DaemonSet은 다음과 같은 용도로 많이 사용된다.

1. **모니터링 에이전트**
    - 각 노드에서 로그를 수집하거나, 메트릭 데이터를 수집하는 에이전트를 배포할 때 사용함.
    - 예: Fluentd, Prometheus Node Exporter
2. **kube-proxy 배포**
    - Kubernetes 네트워크의 핵심 구성 요소인 kube-proxy는 각 노드에서 실행되어야 하므로, DaemonSet으로 배포됨.
3. **네트워크 플러그인**
    - weave-net, calico, flannel 등 네트워크 솔루션은 모든 노드에 네트워크 에이전트를 설치해야 하므로, 역시 DaemonSet을 사용함.

## DaemonSet 정의 구조

DaemonSet 정의 파일은 ReplicaSet과 거의 동일한 구조를 가진다.

다만 `kind`가 `DaemonSet`이라는 점이 다르다.

예시:

```yaml

apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: monitoring-agent
spec:
  selector:
    matchLabels:
      app: monitor
  template:
    metadata:
      labels:
        app: monitor
    spec:
      containers:
        - name: monitor
          image: monitoring-agent:latest

```

- `selector`와 `template.metadata.labels`의 값이 일치해야 한다.
- `spec.template.spec` 안에는 일반적인 Pod 사양과 동일하게 container를 정의한다.

## DaemonSet 생성 및 조회

DaemonSet 리소스는 다음 명령어로 생성하고 조회할 수 있다.

```bash

kubectl apply -f daemonset.yaml
kubectl get daemonset
kubectl describe daemonset <daemonset-name>

```

`kubectl describe` 명령을 통해 각 노드에 몇 개의 Pod이 배치되었는지, 현재 상태는 어떤지 확인할 수 있다.

## 작동 방식

Kubernetes 1.12 이전까지는 DaemonSet이 기본 스케줄러를 사용하지 않고 직접 노드에 Pod을 배치하기 위해 `nodeName` 필드를 명시하는 방식이었다.

```yaml

spec:
  nodeName: node1

```

하지만 Kubernetes 1.12 이후부터는 기본 스케줄러와 **Node Affinity**를 함께 사용하는 방식으로 변경되었다.

이는 스케줄링 로직의 일관성과 유연성을 확보하기 위함이다.

즉, DaemonSet은 내부적으로 각 노드에 적절한 조건을 자동 부여하며, Pod이 해당 노드에 배치되도록 조정한다.

## 정리

| 항목 | 설명 |
| --- | --- |
| 배포 방식 | 클러스터의 **모든 노드에 1개씩** Pod 배포 |
| 사용 목적 | 로그 수집, 모니터링, 네트워크 에이전트 등 |
| 스케줄링 기준 | 기본 스케줄러 + 노드 친화성 기반 |
| 관리 자동화 | 노드 추가/삭제 시 자동으로 Pod 생성/삭제 |
| 정의 구조 | ReplicaSet과 유사, `kind: DaemonSet` 사용 |

---

# Static Pod (정적 포드)

## 정적 포드란?

정적 포드(Static Pod)는 **Kubelet이 직접 관리하는 Pod**로, 일반적으로 Kubernetes API 서버의 명령 없이도 실행된다.

이는 **제어 플레인 없이도 노드 수준에서 Pod을 실행할 수 있는 방법**이며, API 서버가 없는 단일 노드 환경 또는 클러스터 초기 설정에서 자주 사용된다.

## 기본 구조

- 정적 포드는 `kubectl`로 생성하지 않는다.
- Pod 정의 YAML 파일을 Kubelet이 감시하는 디렉토리에 직접 배치함으로써 생성된다.
- 기본 디렉토리 경로는 보통 `/etc/kubernetes/manifests/`이며, 이는 Kubelet 설정에서 `-pod-manifest-path` 옵션으로 지정되어 있다.

```bash

--pod-manifest-path=/etc/kubernetes/manifests

```

이 디렉토리에 정의 파일을 저장하면, Kubelet이 주기적으로 감시하여 자동으로 Pod을 생성하거나 제거한다.

## 동작 방식

- Pod 정의 파일이 **추가되면** → Pod 생성
- Pod 정의 파일이 **수정되면** → Pod 재시작
- Pod 정의 파일이 **삭제되면** → Pod 종료

Kubelet은 이러한 포드들을 자체적으로 유지하며, Pod이 실패하더라도 자동으로 재시작시킨다.

## 정적 포드의 관리 방법

정적 포드는 일반 Kubernetes Pod과 달리 API 서버를 통해 직접 관리할 수 없다.

하지만 클러스터에 연결된 경우, Kubelet은 해당 정적 포드를 **Mirror Pod** 형태로 API 서버에 등록한다.

- Mirror Pod은 읽기 전용이며, `kubectl delete` 명령으로 제거할 수 없다.
- 삭제하려면 원본 정적 Pod 정의 파일을 디렉토리에서 제거해야 한다.

Mirror Pod 이름에는 노드 이름이 붙는다. 예: `nginx-node01`

## kubectl이 작동하지 않는 이유

정적 포드는 Kubelet에서 직접 관리되며, API 서버 없이도 작동할 수 있다.

따라서 `kubectl` 명령어는 사용할 수 없으며, 대신 `docker ps` 또는 `crictl ps` 등의 로컬 명령어로 확인해야 한다.

단, 클러스터에 연결된 상태라면 `kubectl get pods` 명령을 통해 Mirror Pod 상태를 읽을 수는 있다.

## 정적 포드 사용 목적

1. **클러스터 초기 부트스트랩**
    - API 서버, Controller Manager, Scheduler 등 **제어 플레인 구성 요소**를 Pod 형태로 배포할 때 사용됨
    - 예: kubeadm은 마스터 노드에 `/etc/kubernetes/manifests/` 디렉토리를 이용해 Static Pod으로 제어 구성 요소를 설정
2. **고립된 노드에서의 Pod 실행**
    - 클러스터 외부에 있거나, 네트워크가 단절된 환경에서도 Pod을 운영할 수 있음

---

## Static Pod vs DaemonSet

| 항목 | Static Pod | DaemonSet |
| --- | --- | --- |
| 생성 주체 | Kubelet 자체 | Controller에 의해 생성됨 |
| API 서버 필요 여부 | 불필요 | 필요 |
| 제어 방법 | 정의 파일 직접 수정 | `kubectl`로 생성/수정/삭제 가능 |
| 일반 용도 | 제어 플레인 구성 요소 실행 | 노드당 하나씩 실행되는 일반 애플리케이션 |

※ DaemonSet은 클러스터의 모든 노드에 Pod을 자동 배포하는 리소스이지만, 동작은 Controller가 관리한다.

반면 Static Pod은 Kubelet이 직접 제어하므로 API 서버나 Scheduler의 간섭 없이도 작동한다.

## 실습 시 주의할 점

- Pod을 삭제하거나 수정하려면 반드시 해당 파일을 편집하거나 제거해야 한다.
- `kubectl`로는 생성 및 삭제할 수 없고, Mirror Pod만 읽기 전용으로 조회된다.
- 설정 위치를 확인하려면 `kubelet`의 서비스 정의나 설정 파일(`kubelet.conf`)에서 `-pod-manifest-path` 또는 `staticPodPath` 항목을 조회해야 한다.

## 정리

| 항목 | 설명 |
| --- | --- |
| 생성 방식 | 정의 파일을 Kubelet 감시 디렉토리에 직접 배치 |
| API 서버 의존성 | 없음 |
| 재시작 및 유지 관리 | Kubelet이 직접 수행 |
| Mirror Pod 역할 | API 서버에 읽기 전용 등록 |
| 주요 용도 | 마스터 노드 구성 요소 배포, 단일 노드 운영 |
| 삭제 방법 | 디렉토리에서 파일 제거 |

Static Pod은 Kubernetes 클러스터 내부 구성 요소를 안정적으로 구동하고 유지하기 위한 기초 기술이다.

실제 운영 환경에서 `kube-apiserver`, `kube-scheduler`, `etcd` 등이 모두 Static Pod 형태로 배포되는 경우가 많다.

---

## 우선순위 클래스 (PriorityClass)

Kubernetes에서는 다양한 우선순위를 가진 파드(Pod)가 클러스터 내에서 동시에 실행된다.

특히 Kubernetes의 핵심 구성 요소(Control Plane)는 어떤 경우에도 항상 실행되어야 하므로, 다른 일반 애플리케이션보다 높은 우선순위를 가져야 한다.

### 1. PriorityClass란?

우선순위 클래스는 파드의 **우선순위(priority)를 정의하는 객체**이다.

- 파드 간의 우선순위를 정하여 **중요한 워크로드가 우선적으로 스케줄링**되도록 보장한다.
- 예: 시스템 핵심 컴포넌트, 고가용 DB, 일반 애플리케이션, 백그라운드 작업 등은 서로 다른 우선순위를 가져야 한다.

### 2. 동작 방식

- 우선순위 값이 **높을수록 먼저 스케줄링**된다.
- 스케줄러는 클러스터에 리소스가 부족할 경우, **낮은 우선순위를 가진 파드를 종료(evict)**하고 높은 우선순위 파드를 실행시킨다.

### 3. PriorityClass 구성

```yaml

apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 1000000
globalDefault: false
description: "High priority workloads"
preemptionPolicy: PreemptLowerPriority

```

- `value`: 우선순위 값 (숫자가 클수록 높음. 일반적으로 `2,000,000` ~ `1,000,000,000` 범위)
- `globalDefault`: 파드에 명시된 우선순위 클래스가 없을 경우 사용할 기본 클래스 여부
- `preemptionPolicy`: 낮은 우선순위 파드를 선점할지 여부
    - `PreemptLowerPriority`: 기본값. 낮은 우선순위 파드를 제거하고 새 파드를 실행함
    - `Never`: 기존 파드를 제거하지 않고 리소스가 생길 때까지 대기함

### 4. 기본 제공 PriorityClass

| 클래스 이름 | 우선순위 값 | 설명 |
| --- | --- | --- |
| `system-node-critical` | 2000000000 | 노드에 필요한 시스템 파드 |
| `system-cluster-critical` | 2000000000 | 클러스터에 필요한 시스템 파드 |
- 이 두 클래스는 Kubernetes 내부 시스템 컴포넌트를 위한 것이며, 항상 최우선적으로 실행된다.

### 5. 파드에 PriorityClass 적용

```yaml

apiVersion: v1
kind: Pod
metadata:
  name: important-app
spec:
  priorityClassName: high-priority
  containers:
    - name: main
      image: nginx

```

- `priorityClassName` 필드를 통해 파드가 사용할 우선순위 클래스를 지정할 수 있다.

### 6. 우선순위 클래스 생성 시 주의사항

- `globalDefault: true`는 하나의 PriorityClass에서만 가능하다.
- 클러스터 내에 **여러 개의 globalDefault가 존재할 수 없다.**
- `kubectl get priorityclass` 명령어로 현재 정의된 우선순위 클래스를 확인할 수 있다.

### ✅ 요약

- 우선순위 클래스는 중요 워크로드의 안정적 스케줄링을 보장하기 위한 Kubernetes의 기능이다.
- 높은 우선순위 파드는 리소스 부족 시 낮은 우선순위 파드를 선점하여 실행된다.
- 필요 시 `preemptionPolicy`를 설정하여 선점 방지 또는 허용을 조정할 수 있다.

---